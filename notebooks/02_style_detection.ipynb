{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"..\")\n",
    "from src.settings import StyleSettings\n",
    "from src.data.data_tools import StyleDataset\n",
    "import numpy as np\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = StyleSettings()\n",
    "traindataset = StyleDataset([settings.trainpath])\n",
    "testdataset = StyleDataset([settings.testpath])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 419 batches in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "419"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(traindataset) // 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Lace is an openwork fabric , patterned with open holes in the work , made by machine or by hand.',\n",
       " 'wiki')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = traindataset[42]\n",
    "x, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every batch is a `Tuple[str, str]` of a sentence and a label. We can see this is a classification task.\n",
    "The task is, to classify sentences in four categories.\n",
    "Lets build a vocabulary by copy-pasting the code we used before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-25 09:49:44.986 | INFO     | src.models.tokenizer:build_vocab:27 - Found 19306 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "19308"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.models import tokenizer\n",
    "\n",
    "corpus = []\n",
    "for i in range(len(traindataset)):\n",
    "    x = tokenizer.clean(traindataset[i][0])\n",
    "    corpus.append(x)\n",
    "v = tokenizer.build_vocab(corpus, max=20000)\n",
    "len(v)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to cast the labels to an integers. You can use this dictionary to do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {\"humor\": 0, \"reuters\": 1, \"wiki\": 2, \"proverbs\": 3}\n",
    "d[y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1\n",
    "Figure out, for every class, what accuracy you should expect if the model would guess blind on the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'humor': 0.31414510476474533,\n",
       " 'wiki': 0.31175900380284843,\n",
       " 'proverbs': 0.06196405935426143,\n",
       " 'reuters': 0.3121318320781448}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import collections\n",
    "count_labels = collections.Counter([traindataset[i][1] for  i in range(len(traindataset))])\n",
    "{v: count_labels[v] / len(traindataset) for v in count_labels}\n",
    "# TODO ~ about 4 lines of code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Geen normaal verdeling > weinig voorbeelden, check embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reflect on what you see. What does this mean? What implications does this have? Why is that good/bad?\n",
    "Are there things down the line that could cause a problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2 : Implement a preprocessor\n",
    "\n",
    "We can inherit from `tokenizer.Preprocessor`\n",
    "Only thing we need to adjust is the `cast_label` function.\n",
    " \n",
    "- create a StylePreprocessor class\n",
    "- inherit from Preprocessor\n",
    "- create a new cast_label function for this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO ~ about 4 lines of code\n",
    "class StylePreprocessor(tokenizer.Preprocessor):\n",
    "    def cast_label(self, label: str) -> int:\n",
    "        if label in d.keys():\n",
    "            return d[label]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test the preprocessor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = StylePreprocessor(max=100, vocab=v, clean=tokenizer.clean)\n",
    "x=preprocessor([(x, y)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[1][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up the model\n",
    "We can re-use the BaseDatastreamer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data import data_tools\n",
    "\n",
    "trainstreamer = data_tools.BaseDatastreamer(\n",
    "    dataset=traindataset, batchsize=32, preprocessor=preprocessor\n",
    ").stream()\n",
    "teststreamer = data_tools.BaseDatastreamer(\n",
    "    dataset=testdataset, batchsize=32, preprocessor=preprocessor\n",
    ").stream()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[11202,  5398,  7658,  ...,     0,     0,     0],\n",
       "        [   63,  3098,   251,  ...,     0,     0,     0],\n",
       "        [   89,     3,    62,  ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [ 1486,  1062,   106,  ...,     0,     0,     0],\n",
       "        [    2,    24,   329,  ...,     0,     0,     0],\n",
       "        [  538,     8,    53,  ...,     0,     0,     0]], dtype=torch.int32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = next(trainstreamer)\n",
    "x.shape, y\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3 : Metrics, loss\n",
    "Select proper metrics and a loss function.\n",
    "\n",
    "Bonus: implement an additional metric function that is relevant for this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models import metrics\n",
    "import torch\n",
    "# TODO ~ 2 lines of code\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "metrics = [metrics.Accuracy()]\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Opbasis van theorie MASE en crossentropyloss(). Next step misschien mogelijk metrics 2 toevoegen. Keuze accuracy > mogelijk meer groei in > literatuur nog opzoeken. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4 : Basemodel\n",
    "Create a base model. It does not need to be naive; you could re-use the\n",
    "NLP models we used for the IMDB.\n",
    "\n",
    "I suggest to start with a hidden size of about 128.\n",
    "Use a config dictionary, or a gin file, both are fine.\n",
    "\n",
    "Bonus points if you create a Trax model in src.models, and even more if you add a trax training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "log_dir = settings.log_dir\n",
    "# TODO between 2 and 8 lines of code, depending on your setup\n",
    "# Assuming you load your model in one line of code from src.models.rnn\n",
    "config = {\n",
    "    \"vocab\": len(v),\n",
    "    \"hidden_size\": 512,\n",
    "    \"num_layers\": 3, # check num_layers\n",
    "    \"dropout\": 0.1, # check dropout\n",
    "    \"output_size\": 4,\n",
    "}\n",
    "from src.models import rnn\n",
    "model = rnn.NLPmodel(config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the trainloop.\n",
    "\n",
    "- Give the lenght of the traindataset, how many batches of 32 can you get out of it?\n",
    "- If you take a short amount of train_steps (eg 25) for every epoch, how many epochs do you need to cover the complete dataset?\n",
    "- What amount of epochs do you need to run the loop with trainsteps=25 to cover the complete traindataset once? \n",
    "- answer the questions above, and pick a reasonable epoch lenght\n",
    "\n",
    "Start with a default learning_rate of 1e-3 and an Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "419.09375"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(traindataset)/32 # lengte traindataset gedeeld door de batchsize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dus je hebt 420 batches van 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.955"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(len(traindataset) + len(testdataset)) / (32*25) # later nog checken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "21 epoches dus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "419.09375"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(traindataset) / (32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.8"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "420/25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minimaal 17 epochs dus op basis van 25 train_steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-25 09:49:50.337042: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-06-25 09:49:50.337075: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-06-25 09:50:03.972 | INFO     | src.data.data_tools:dir_add_timestamp:66 - Logging to ../tune/20220625-0950\n",
      " 60%|██████    | 15/25 [00:02<00:01,  7.30it/s]\n",
      "  0%|          | 0/17 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/mladmin/code/examen-22/notebooks/02_style_detection.ipynb Cell 37'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bml-lab-499b0cc0-f0fa-444d-a905-b2d2bead7477.westeurope.cloudapp.azure.com/home/mladmin/code/examen-22/notebooks/02_style_detection.ipynb#ch0000036vscode-remote?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtraining\u001b[39;00m \u001b[39mimport\u001b[39;00m train_model\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bml-lab-499b0cc0-f0fa-444d-a905-b2d2bead7477.westeurope.cloudapp.azure.com/home/mladmin/code/examen-22/notebooks/02_style_detection.ipynb#ch0000036vscode-remote?line=2'>3</a>\u001b[0m model \u001b[39m=\u001b[39m train_model\u001b[39m.\u001b[39;49mtrainloop(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bml-lab-499b0cc0-f0fa-444d-a905-b2d2bead7477.westeurope.cloudapp.azure.com/home/mladmin/code/examen-22/notebooks/02_style_detection.ipynb#ch0000036vscode-remote?line=3'>4</a>\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m17\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bml-lab-499b0cc0-f0fa-444d-a905-b2d2bead7477.westeurope.cloudapp.azure.com/home/mladmin/code/examen-22/notebooks/02_style_detection.ipynb#ch0000036vscode-remote?line=4'>5</a>\u001b[0m     model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bml-lab-499b0cc0-f0fa-444d-a905-b2d2bead7477.westeurope.cloudapp.azure.com/home/mladmin/code/examen-22/notebooks/02_style_detection.ipynb#ch0000036vscode-remote?line=5'>6</a>\u001b[0m     metrics\u001b[39m=\u001b[39;49mmetrics,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bml-lab-499b0cc0-f0fa-444d-a905-b2d2bead7477.westeurope.cloudapp.azure.com/home/mladmin/code/examen-22/notebooks/02_style_detection.ipynb#ch0000036vscode-remote?line=6'>7</a>\u001b[0m     optimizer\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49moptim\u001b[39m.\u001b[39;49mAdam,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bml-lab-499b0cc0-f0fa-444d-a905-b2d2bead7477.westeurope.cloudapp.azure.com/home/mladmin/code/examen-22/notebooks/02_style_detection.ipynb#ch0000036vscode-remote?line=7'>8</a>\u001b[0m     learning_rate\u001b[39m=\u001b[39;49m\u001b[39m1e-3\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bml-lab-499b0cc0-f0fa-444d-a905-b2d2bead7477.westeurope.cloudapp.azure.com/home/mladmin/code/examen-22/notebooks/02_style_detection.ipynb#ch0000036vscode-remote?line=8'>9</a>\u001b[0m     loss_fn\u001b[39m=\u001b[39;49mloss_fn,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bml-lab-499b0cc0-f0fa-444d-a905-b2d2bead7477.westeurope.cloudapp.azure.com/home/mladmin/code/examen-22/notebooks/02_style_detection.ipynb#ch0000036vscode-remote?line=9'>10</a>\u001b[0m     train_dataloader\u001b[39m=\u001b[39;49mtrainstreamer,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bml-lab-499b0cc0-f0fa-444d-a905-b2d2bead7477.westeurope.cloudapp.azure.com/home/mladmin/code/examen-22/notebooks/02_style_detection.ipynb#ch0000036vscode-remote?line=10'>11</a>\u001b[0m     test_dataloader\u001b[39m=\u001b[39;49mteststreamer,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bml-lab-499b0cc0-f0fa-444d-a905-b2d2bead7477.westeurope.cloudapp.azure.com/home/mladmin/code/examen-22/notebooks/02_style_detection.ipynb#ch0000036vscode-remote?line=11'>12</a>\u001b[0m     log_dir\u001b[39m=\u001b[39;49mlog_dir,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bml-lab-499b0cc0-f0fa-444d-a905-b2d2bead7477.westeurope.cloudapp.azure.com/home/mladmin/code/examen-22/notebooks/02_style_detection.ipynb#ch0000036vscode-remote?line=12'>13</a>\u001b[0m     train_steps\u001b[39m=\u001b[39;49m\u001b[39m25\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bml-lab-499b0cc0-f0fa-444d-a905-b2d2bead7477.westeurope.cloudapp.azure.com/home/mladmin/code/examen-22/notebooks/02_style_detection.ipynb#ch0000036vscode-remote?line=13'>14</a>\u001b[0m     eval_steps\u001b[39m=\u001b[39;49m\u001b[39m25\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bml-lab-499b0cc0-f0fa-444d-a905-b2d2bead7477.westeurope.cloudapp.azure.com/home/mladmin/code/examen-22/notebooks/02_style_detection.ipynb#ch0000036vscode-remote?line=14'>15</a>\u001b[0m )\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/exam-22-DDG3aTJy-py3.9/lib/python3.9/site-packages/gin/config.py:1582\u001b[0m, in \u001b[0;36m_make_gin_wrapper.<locals>.gin_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1579\u001b[0m new_kwargs\u001b[39m.\u001b[39mupdate(kwargs)\n\u001b[1;32m   1581\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1582\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49mnew_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mnew_kwargs)\n\u001b[1;32m   1583\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m   1584\u001b[0m   err_str \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\n",
      "File \u001b[0;32m~/code/examen-22/notebooks/../src/training/train_model.py:139\u001b[0m, in \u001b[0;36mtrainloop\u001b[0;34m(epochs, model, optimizer, learning_rate, loss_fn, metrics, train_dataloader, test_dataloader, log_dir, train_steps, eval_steps, patience, factor, tunewriter)\u001b[0m\n\u001b[1;32m    136\u001b[0m     write_gin(log_dir)\n\u001b[1;32m    138\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(epochs)):\n\u001b[0;32m--> 139\u001b[0m     train_loss \u001b[39m=\u001b[39m trainbatches(\n\u001b[1;32m    140\u001b[0m         model, train_dataloader, loss_fn, optimizer_, train_steps\n\u001b[1;32m    141\u001b[0m     )\n\u001b[1;32m    143\u001b[0m     metric_dict, test_loss \u001b[39m=\u001b[39m evalbatches(\n\u001b[1;32m    144\u001b[0m         model, test_dataloader, loss_fn, metrics, eval_steps\n\u001b[1;32m    145\u001b[0m     )\n\u001b[1;32m    147\u001b[0m     scheduler\u001b[39m.\u001b[39mstep(test_loss)\n",
      "File \u001b[0;32m~/code/examen-22/notebooks/../src/training/train_model.py:45\u001b[0m, in \u001b[0;36mtrainbatches\u001b[0;34m(model, traindatastreamer, loss_fn, optimizer, train_steps)\u001b[0m\n\u001b[1;32m     43\u001b[0m     loss \u001b[39m=\u001b[39m loss_fn(yhat, y)\n\u001b[1;32m     44\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m---> 45\u001b[0m     optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m     46\u001b[0m     train_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy()\n\u001b[1;32m     47\u001b[0m train_loss \u001b[39m/\u001b[39m\u001b[39m=\u001b[39m train_steps\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/exam-22-DDG3aTJy-py3.9/lib/python3.9/site-packages/torch/optim/optimizer.py:88\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m     87\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m---> 88\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/exam-22-DDG3aTJy-py3.9/lib/python3.9/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/exam-22-DDG3aTJy-py3.9/lib/python3.9/site-packages/torch/optim/adam.py:141\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[39m# record the step after step update\u001b[39;00m\n\u001b[1;32m    139\u001b[0m             state_steps\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m--> 141\u001b[0m     F\u001b[39m.\u001b[39;49madam(params_with_grad,\n\u001b[1;32m    142\u001b[0m            grads,\n\u001b[1;32m    143\u001b[0m            exp_avgs,\n\u001b[1;32m    144\u001b[0m            exp_avg_sqs,\n\u001b[1;32m    145\u001b[0m            max_exp_avg_sqs,\n\u001b[1;32m    146\u001b[0m            state_steps,\n\u001b[1;32m    147\u001b[0m            amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    148\u001b[0m            beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    149\u001b[0m            beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    150\u001b[0m            lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    151\u001b[0m            weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    152\u001b[0m            eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    153\u001b[0m            maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m    154\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/exam-22-DDG3aTJy-py3.9/lib/python3.9/site-packages/torch/optim/_functional.py:97\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m     94\u001b[0m     grad \u001b[39m=\u001b[39m grad\u001b[39m.\u001b[39madd(param, alpha\u001b[39m=\u001b[39mweight_decay)\n\u001b[1;32m     96\u001b[0m \u001b[39m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[0;32m---> 97\u001b[0m exp_avg\u001b[39m.\u001b[39;49mmul_(beta1)\u001b[39m.\u001b[39madd_(grad, alpha\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m beta1)\n\u001b[1;32m     98\u001b[0m exp_avg_sq\u001b[39m.\u001b[39mmul_(beta2)\u001b[39m.\u001b[39maddcmul_(grad, grad\u001b[39m.\u001b[39mconj(), value\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m beta2)\n\u001b[1;32m     99\u001b[0m \u001b[39mif\u001b[39;00m amsgrad:\n\u001b[1;32m    100\u001b[0m     \u001b[39m# Maintains the maximum of all 2nd moment running avg. till now\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from src.training import train_model\n",
    "\n",
    "model = train_model.trainloop(\n",
    "    epochs=17,\n",
    "    model=model,\n",
    "    metrics=metrics,\n",
    "    optimizer=torch.optim.Adam,\n",
    "    learning_rate=1e-3,\n",
    "    loss_fn=loss_fn,\n",
    "    train_dataloader=trainstreamer,\n",
    "    test_dataloader=teststreamer,\n",
    "    log_dir=log_dir,\n",
    "    train_steps=25,\n",
    "    eval_steps=25,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save an image from the training in tensorboard in the `figures` folder.\n",
    "Explain what you are seeing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "your explanation here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Evaluate the basemodel\n",
    "Create a confusion matrix with the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Text(0.5, 15.0, 'Predicted'), Text(33.0, 0.5, 'Target')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for _ in range(10):\n",
    "    X, y = next(teststreamer)\n",
    "    yhat = model(X)\n",
    "    yhat = yhat.argmax(dim=1)\n",
    "    y_pred.append(yhat.tolist())\n",
    "    y_true.append(y.tolist())\n",
    "\n",
    "yhat = [x for y in y_pred for x in y]\n",
    "y = [x for y in y_true for x in y]\n",
    "\n",
    "cfm = confusion_matrix(y, yhat)\n",
    "cfm_norm = cfm / np.sum(cfm, axis=1, keepdims=True)\n",
    "plot = sns.heatmap(cfm_norm, annot=cfm_norm, fmt=\".3f\")\n",
    "plot.set(xlabel=\"Predicted\", ylabel=\"Target\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save this in the figures folder.\n",
    "Interpret this. \n",
    "\n",
    "- What is going on?\n",
    "- What is a good metric here?\n",
    "- how is your answer to Q1 relevant here?\n",
    "- Is there something you could do to fix/improve things, after you see these results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Je ziet hier de uitkomst tussen neit normale verdeling van de dataset. Mogleijk datamodificeren zodat er meer data van die groep binnenkomt. Geen data weggooien. Misschien meer gewicht meegen aan de groep "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Tune the model\n",
    "Don't overdo this.\n",
    "More is not better.\n",
    "\n",
    "Bonus points for things like:\n",
    "- Attention layers\n",
    "- Trax architecture including a functioning training loop\n",
    "\n",
    "Keep it small! It's better to present 2 or 3 sane experiments that are structured and thought trough, than 25 random guesses. You can test more, but select 2 or 3 of the best alternatives you researched, with a rationale why this works better.\n",
    "\n",
    "Keep it concise; explain:\n",
    "- what you changed\n",
    "- why you thought that was a good idea  \n",
    "- what the impact was (visualise or numeric)\n",
    "- explain the impact\n",
    "\n",
    "You dont need to get a perfect score; curiousity driven research that fails is fine.\n",
    "The insight into what is happening is more important than the quantity.\n",
    "\n",
    "Keep logs of your settings;\n",
    "either use gin, or save configs, or both :)\n",
    "Store images in the `figures` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter groter maken, le groter maken ( is nu vrij klein) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hierbij hebben we een nieuwe class toegevoegd in de rnn models. Dit is de AttentionNLP > hierbij zit dus een attional layer tussen de GRU en linear. Dit zorgt voor meer contest in het verhaal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = rnn.AttentionNLP(config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "20220625-0936 zonder attention layer\n",
    "20220625-0938 met attenlion layer. Zorg voor lagere loss en hogere accurcy. \n",
    "Nog to do learnin grate spelen en trains steps. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test op meer filters om 12:05 > zorgt mogelijk over overfitting > telkens copies maken. Met meer filters wel hogere accurcy direct in het begin (20220625-0957)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-25 09:57:31.194 | INFO     | src.data.data_tools:dir_add_timestamp:66 - Logging to ../tune/20220625-0957\n",
      "100%|██████████| 25/25 [00:11<00:00,  2.23it/s]\n",
      "2022-06-25 09:57:44.853 | INFO     | src.training.train_model:trainloop:164 - Epoch 0 train 1.0285 test 0.7756 metric ['0.7412']\n",
      "100%|██████████| 25/25 [00:09<00:00,  2.60it/s]\n",
      "2022-06-25 09:57:56.838 | INFO     | src.training.train_model:trainloop:164 - Epoch 1 train 0.7888 test 0.6402 metric ['0.7887']\n",
      "100%|██████████| 25/25 [00:09<00:00,  2.64it/s]\n",
      "2022-06-25 09:58:08.678 | INFO     | src.training.train_model:trainloop:164 - Epoch 2 train 0.6609 test 0.5061 metric ['0.8213']\n",
      "100%|██████████| 25/25 [00:09<00:00,  2.52it/s]\n",
      "2022-06-25 09:58:20.885 | INFO     | src.training.train_model:trainloop:164 - Epoch 3 train 0.5575 test 0.5062 metric ['0.8213']\n",
      "100%|██████████| 25/25 [00:09<00:00,  2.72it/s]\n",
      "2022-06-25 09:58:32.592 | INFO     | src.training.train_model:trainloop:164 - Epoch 4 train 0.5051 test 0.4795 metric ['0.8200']\n",
      "100%|██████████| 25/25 [00:09<00:00,  2.59it/s]\n",
      "2022-06-25 09:58:44.375 | INFO     | src.training.train_model:trainloop:164 - Epoch 5 train 0.4922 test 0.4225 metric ['0.8462']\n",
      "100%|██████████| 25/25 [00:09<00:00,  2.69it/s]\n",
      "2022-06-25 09:58:56.016 | INFO     | src.training.train_model:trainloop:164 - Epoch 6 train 0.4650 test 0.4369 metric ['0.8450']\n",
      "100%|██████████| 25/25 [00:09<00:00,  2.65it/s]\n",
      "2022-06-25 09:59:08.003 | INFO     | src.training.train_model:trainloop:164 - Epoch 7 train 0.4285 test 0.4362 metric ['0.8625']\n",
      "100%|██████████| 25/25 [00:09<00:00,  2.62it/s]\n",
      "2022-06-25 09:59:20.221 | INFO     | src.training.train_model:trainloop:164 - Epoch 8 train 0.4596 test 0.4087 metric ['0.8588']\n",
      "100%|██████████| 25/25 [00:10<00:00,  2.50it/s]\n",
      "2022-06-25 09:59:32.809 | INFO     | src.training.train_model:trainloop:164 - Epoch 9 train 0.3264 test 0.3975 metric ['0.8662']\n",
      "100%|██████████| 25/25 [00:08<00:00,  2.89it/s]\n",
      "2022-06-25 09:59:44.759 | INFO     | src.training.train_model:trainloop:164 - Epoch 10 train 0.3622 test 0.3811 metric ['0.8700']\n",
      "100%|██████████| 25/25 [00:08<00:00,  2.84it/s]\n",
      "2022-06-25 09:59:56.898 | INFO     | src.training.train_model:trainloop:164 - Epoch 11 train 0.3406 test 0.4077 metric ['0.8600']\n",
      "100%|██████████| 25/25 [00:09<00:00,  2.70it/s]\n",
      "2022-06-25 10:00:08.300 | INFO     | src.training.train_model:trainloop:164 - Epoch 12 train 0.3147 test 0.4190 metric ['0.8725']\n",
      "100%|██████████| 25/25 [00:09<00:00,  2.63it/s]\n",
      "2022-06-25 10:00:20.348 | INFO     | src.training.train_model:trainloop:164 - Epoch 13 train 0.3245 test 0.4418 metric ['0.8562']\n",
      "100%|██████████| 25/25 [00:08<00:00,  2.93it/s]\n",
      "2022-06-25 10:00:31.378 | INFO     | src.training.train_model:trainloop:164 - Epoch 14 train 0.3360 test 0.4038 metric ['0.8750']\n",
      "100%|██████████| 25/25 [00:09<00:00,  2.77it/s]\n",
      "2022-06-25 10:00:42.561 | INFO     | src.training.train_model:trainloop:164 - Epoch 15 train 0.3340 test 0.3189 metric ['0.8862']\n",
      "100%|██████████| 25/25 [00:08<00:00,  2.80it/s]\n",
      "2022-06-25 10:00:53.824 | INFO     | src.training.train_model:trainloop:164 - Epoch 16 train 0.2993 test 0.3773 metric ['0.8588']\n",
      "100%|██████████| 25/25 [00:09<00:00,  2.62it/s]\n",
      "2022-06-25 10:01:05.679 | INFO     | src.training.train_model:trainloop:164 - Epoch 17 train 0.3064 test 0.3596 metric ['0.8788']\n",
      "100%|██████████| 25/25 [00:09<00:00,  2.74it/s]\n",
      "2022-06-25 10:01:17.379 | INFO     | src.training.train_model:trainloop:164 - Epoch 18 train 0.2212 test 0.4837 metric ['0.8825']\n",
      "100%|██████████| 25/25 [00:09<00:00,  2.69it/s]\n",
      "2022-06-25 10:01:28.987 | INFO     | src.training.train_model:trainloop:164 - Epoch 19 train 0.3174 test 0.3885 metric ['0.8625']\n",
      "100%|██████████| 25/25 [00:09<00:00,  2.77it/s]\n",
      "2022-06-25 10:01:41.746 | INFO     | src.training.train_model:trainloop:164 - Epoch 20 train 0.2875 test 0.3298 metric ['0.8912']\n",
      "100%|██████████| 25/25 [00:08<00:00,  2.94it/s]\n",
      "2022-06-25 10:01:52.652 | INFO     | src.training.train_model:trainloop:164 - Epoch 21 train 0.2901 test 0.4273 metric ['0.8438']\n",
      "100%|██████████| 25/25 [00:09<00:00,  2.75it/s]\n",
      "2022-06-25 10:02:03.966 | INFO     | src.training.train_model:trainloop:164 - Epoch 22 train 0.2830 test 0.3865 metric ['0.8750']\n",
      "100%|██████████| 25/25 [00:08<00:00,  2.97it/s]\n",
      "2022-06-25 10:02:14.691 | INFO     | src.training.train_model:trainloop:164 - Epoch 23 train 0.2433 test 0.3078 metric ['0.8888']\n",
      "100%|██████████| 25/25 [00:09<00:00,  2.58it/s]\n",
      "2022-06-25 10:02:26.929 | INFO     | src.training.train_model:trainloop:164 - Epoch 24 train 0.2945 test 0.3403 metric ['0.8862']\n",
      "100%|██████████| 25/25 [00:08<00:00,  2.78it/s]\n",
      "2022-06-25 10:02:38.355 | INFO     | src.training.train_model:trainloop:164 - Epoch 25 train 0.3314 test 0.3095 metric ['0.8938']\n",
      "100%|██████████| 25/25 [00:08<00:00,  3.02it/s]\n",
      "2022-06-25 10:02:48.880 | INFO     | src.training.train_model:trainloop:164 - Epoch 26 train 0.1542 test 0.4628 metric ['0.8775']\n",
      "100%|██████████| 25/25 [00:08<00:00,  2.79it/s]\n",
      "2022-06-25 10:03:00.010 | INFO     | src.training.train_model:trainloop:164 - Epoch 27 train 0.1658 test 0.3556 metric ['0.8988']\n",
      "100%|██████████| 25/25 [00:09<00:00,  2.71it/s]\n",
      "2022-06-25 10:03:11.878 | INFO     | src.training.train_model:trainloop:164 - Epoch 28 train 0.1434 test 0.4441 metric ['0.8788']\n",
      "100%|██████████| 25/25 [00:08<00:00,  3.11it/s]\n",
      "2022-06-25 10:03:22.235 | INFO     | src.training.train_model:trainloop:164 - Epoch 29 train 0.1991 test 0.3714 metric ['0.8888']\n",
      "100%|██████████| 25/25 [00:07<00:00,  3.32it/s]\n",
      "2022-06-25 10:03:32.293 | INFO     | src.training.train_model:trainloop:164 - Epoch 30 train 0.1511 test 0.3288 metric ['0.8975']\n",
      "100%|██████████| 25/25 [00:07<00:00,  3.32it/s]\n",
      "2022-06-25 10:03:42.300 | INFO     | src.training.train_model:trainloop:164 - Epoch 31 train 0.1422 test 0.3928 metric ['0.8875']\n",
      "100%|██████████| 25/25 [00:07<00:00,  3.47it/s]\n",
      "2022-06-25 10:03:51.780 | INFO     | src.training.train_model:trainloop:164 - Epoch 32 train 0.1620 test 0.3734 metric ['0.9075']\n",
      "100%|██████████| 25/25 [00:07<00:00,  3.51it/s]\n",
      "2022-06-25 10:04:01.193 | INFO     | src.training.train_model:trainloop:164 - Epoch 33 train 0.1725 test 0.3553 metric ['0.9000']\n",
      "100%|██████████| 25/25 [00:07<00:00,  3.49it/s]\n",
      "2022-06-25 10:04:10.657 | INFO     | src.training.train_model:trainloop:164 - Epoch 34 train 0.1314 test 0.3129 metric ['0.9062']\n",
      "100%|██████████| 25/25 [00:07<00:00,  3.41it/s]\n",
      "2022-06-25 10:04:20.254 | INFO     | src.training.train_model:trainloop:164 - Epoch 35 train 0.1603 test 0.3383 metric ['0.8800']\n",
      "100%|██████████| 25/25 [00:08<00:00,  3.04it/s]\n",
      "2022-06-25 10:04:30.883 | INFO     | src.training.train_model:trainloop:164 - Epoch 36 train 0.1623 test 0.4782 metric ['0.8725']\n",
      "100%|██████████| 25/25 [00:07<00:00,  3.17it/s]\n",
      "2022-06-25 10:04:41.347 | INFO     | src.training.train_model:trainloop:164 - Epoch 37 train 0.1423 test 0.3048 metric ['0.9062']\n",
      "100%|██████████| 25/25 [00:07<00:00,  3.37it/s]\n",
      "2022-06-25 10:04:50.956 | INFO     | src.training.train_model:trainloop:164 - Epoch 38 train 0.1348 test 0.3893 metric ['0.8788']\n",
      "100%|██████████| 25/25 [00:07<00:00,  3.26it/s]\n",
      "2022-06-25 10:05:02.255 | INFO     | src.training.train_model:trainloop:164 - Epoch 39 train 0.1373 test 0.3924 metric ['0.8725']\n",
      "100%|██████████| 40/40 [07:31<00:00, 11.28s/it]\n"
     ]
    }
   ],
   "source": [
    "model = train_model.trainloop(\n",
    "    epochs=40,\n",
    "    model=model,\n",
    "    metrics=metrics,\n",
    "    optimizer=torch.optim.Adam,\n",
    "    learning_rate=1e-3,\n",
    "    loss_fn=loss_fn,\n",
    "    train_dataloader=trainstreamer,\n",
    "    test_dataloader=teststreamer,\n",
    "    log_dir=log_dir,\n",
    "    train_steps=25,\n",
    "    eval_steps=25,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('exam-22-DDG3aTJy-py3.9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3ef7eee7c1ffccdb050f8336de9a04a9ab88c4d3eb3bee3e0a27c87a184d1d38"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
