{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"..\")\n",
    "from src.settings import StyleSettings\n",
    "from src.data.data_tools import StyleDataset\n",
    "import numpy as np\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = StyleSettings()\n",
    "traindataset = StyleDataset([settings.trainpath])\n",
    "testdataset = StyleDataset([settings.testpath])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 419 batches in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "419"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(traindataset) // 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Lace is an openwork fabric , patterned with open holes in the work , made by machine or by hand.',\n",
       " 'wiki')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = traindataset[42]\n",
    "x, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every batch is a `Tuple[str, str]` of a sentence and a label. We can see this is a classification task.\n",
    "The task is, to classify sentences in four categories.\n",
    "Lets build a vocabulary by copy-pasting the code we used before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-04 17:45:12.792 | INFO     | src.models.tokenizer:build_vocab:27 - Found 19306 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "19308"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.models import tokenizer\n",
    "\n",
    "corpus = []\n",
    "for i in range(len(traindataset)):\n",
    "    x = tokenizer.clean(traindataset[i][0])\n",
    "    corpus.append(x)\n",
    "v = tokenizer.build_vocab(corpus, max=20000)\n",
    "len(v)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to cast the labels to an integers. You can use this dictionary to do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {\"humor\": 0, \"reuters\": 1, \"wiki\": 2, \"proverbs\": 3}\n",
    "d[y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1\n",
    "Figure out, for every class, what accuracy you should expect if the model would guess blind on the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'humor': 0.31414510476474533,\n",
       " 'wiki': 0.31175900380284843,\n",
       " 'proverbs': 0.06196405935426143,\n",
       " 'reuters': 0.3121318320781448}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import collections\n",
    "number_labels = collections.Counter([traindataset[i][1] for  i in range(len(traindataset))])\n",
    "{v: number_labels[v] / len(traindataset) for v in number_labels}\n",
    "# TODO ~ about 4 lines of code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reflect on what you see. What does this mean? What implications does this have? Why is that good/bad?\n",
    "Are there things down the line that could cause a problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Antwoord"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naar voren komt is dat de dataset ongebalanceerd is. De verdeling van de 4 classes is niet gelijk. Het aantal zinnen met het label 'proverbs' is beduidend lager vergeleken met de andere 3 labels. De kans dat de zin het label 'humor' of 'wiki'of 'reuters' heeft is veel groter. Het model kan hierdoor niet bruikbare features leren van de groep met het label 'proverbs', het model heeft minder voorbeelden ter beschikking om van te leren. Er is hierdoor een vergrote kans van overfitting.      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2 : Implement a preprocessor\n",
    "\n",
    "We can inherit from `tokenizer.Preprocessor`\n",
    "Only thing we need to adjust is the `cast_label` function.\n",
    " \n",
    "- create a StylePreprocessor class\n",
    "- inherit from Preprocessor\n",
    "- create a new cast_label function for this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO ~ about 4 lines of code\n",
    "class StylePreprocessor(tokenizer.Preprocessor):\n",
    "    \"A preprocessor with the cast_label function for adding the labels from the dictionary\"\n",
    "    def cast_label(self, label: str) -> int:\n",
    "        if label in d.keys():\n",
    "            return d[label]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test the preprocessor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[4929,  854,   32,   15,  499,   21, 8496,  890]], dtype=torch.int32),\n",
       " tensor([2]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor = StylePreprocessor(max=100, vocab=v, clean=tokenizer.clean)\n",
    "preprocessor([(x, y)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up the model\n",
    "We can re-use the BaseDatastreamer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data import data_tools\n",
    "\n",
    "trainstreamer = data_tools.BaseDatastreamer(\n",
    "    dataset=traindataset, batchsize=32, preprocessor=preprocessor\n",
    ").stream()\n",
    "teststreamer = data_tools.BaseDatastreamer(\n",
    "    dataset=testdataset, batchsize=32, preprocessor=preprocessor\n",
    ").stream()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 42]),\n",
       " tensor([2, 1, 3, 0, 2, 0, 2, 0, 0, 0, 2, 2, 0, 3, 1, 2, 0, 0, 0, 0, 0, 3, 2, 0,\n",
       "         2, 0, 2, 0, 0, 0, 0, 2]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = next(trainstreamer)\n",
    "x.shape, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3 : Metrics, loss\n",
    "Select proper metrics and a loss function.\n",
    "\n",
    "Bonus: implement an additional metric function that is relevant for this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models import metrics\n",
    "import torch\n",
    "# TODO ~ 2 lines of code\n",
    "\n",
    "metrics = [metrics.Accuracy(), metrics.F1Score()]\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Antwoord"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Om te controleren of de uitkomst correct is gebruiken we de CrossEntropyloss. Deze is geschikt voor multiclass categorisatie. MSE kan bijvoorbeeld hier niet voor gebruiken aangezien dit juist is voor numerieke voorspellingen. Ik heb als addition metric toegevoegd de F1Score toegevoegd. De f1 score kijkt naar een gemiddele van de precisie en recall ( tussen 0 en 1) scores en kan worden gebruikt voor classificatie problemen en in ongebalanceerde datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4 : Basemodel\n",
    "Create a base model. It does not need to be naive; you could re-use the\n",
    "NLP models we used for the IMDB.\n",
    "\n",
    "I suggest to start with a hidden size of about 128.\n",
    "Use a config dictionary, or a gin file, both are fine.\n",
    "\n",
    "Bonus points if you create a Trax model in src.models, and even more if you add a trax training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "log_dir = settings.log_dir\n",
    "# TODO between 2 and 8 lines of code, depending on your setup\n",
    "# Assuming you load your model in one line of code from src.models.rnn\n",
    "config = {\n",
    "    \"vocab\": len(v),\n",
    "    \"hidden_size\": 128,\n",
    "    \"num_layers\": 3, \n",
    "    \"dropout\": 0.1, \n",
    "    \"output_size\": 4,\n",
    "}\n",
    "from src.models import rnn\n",
    "model = rnn.NLPmodel(config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the trainloop.\n",
    "\n",
    "- Give the lenght of the traindataset, how many batches of 32 can you get out of it?\n",
    "- If you take a short amount of train_steps (eg 25) for every epoch, how many epochs do you need to cover the complete dataset?\n",
    "- What amount of epochs do you need to run the loop with trainsteps=25 to cover the complete traindataset once? \n",
    "- answer the questions above, and pick a reasonable epoch lenght\n",
    "\n",
    "Start with a default learning_rate of 1e-3 and an Adam optimizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Antwoord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "419.09375"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(traindataset)/32 # lengte traindataset gedeeld door de batchsize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De lengte van de traindataset is 13.411. Hierbij kun je dus 420 batches maken van 32. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.955"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(len(traindataset) + len(testdataset)) / (32*25) # de totale lengte van de dataset gedeeld door de train step en de batchsize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De totale dataset heeft een lengte van 16.764. Om de hele datsaet te covereren zijn er 21 epochs nodig. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.76375"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(traindataset) / (32*25) # de totale lengte van de traindataset gedeeld door de train step en de batchsize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Om de totale traindataset te coveren zijn er minmaal 17 epochs nodig op basis van 25 train_steps.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-04 17:45:15.574382: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-07-04 17:45:15.574412: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-07-04 17:45:19.038 | INFO     | src.data.data_tools:dir_add_timestamp:66 - Logging to ../tune/20220704-1745\n",
      "100%|██████████| 25/25 [00:04<00:00,  5.22it/s]\n",
      "2022-07-04 17:45:24.552 | INFO     | src.training.train_model:trainloop:164 - Epoch 0 train 1.2970 test 1.2538 metric ['0.3700', '0.2138']\n",
      "100%|██████████| 25/25 [00:02<00:00, 10.24it/s]\n",
      "2022-07-04 17:45:27.550 | INFO     | src.training.train_model:trainloop:164 - Epoch 1 train 1.2798 test 1.2230 metric ['0.3613', '0.2341']\n",
      "100%|██████████| 25/25 [00:01<00:00, 14.55it/s]\n",
      "2022-07-04 17:45:29.692 | INFO     | src.training.train_model:trainloop:164 - Epoch 2 train 1.2221 test 1.1551 metric ['0.4625', '0.3373']\n",
      "100%|██████████| 25/25 [00:01<00:00, 17.06it/s]\n",
      "2022-07-04 17:45:31.570 | INFO     | src.training.train_model:trainloop:164 - Epoch 3 train 1.1019 test 0.9642 metric ['0.5575', '0.3547']\n",
      "100%|██████████| 25/25 [00:01<00:00, 16.03it/s]\n",
      "2022-07-04 17:45:33.527 | INFO     | src.training.train_model:trainloop:164 - Epoch 4 train 0.9110 test 0.8609 metric ['0.6700', '0.5534']\n",
      "100%|██████████| 25/25 [00:01<00:00, 17.71it/s]\n",
      "2022-07-04 17:45:35.364 | INFO     | src.training.train_model:trainloop:164 - Epoch 5 train 0.7690 test 0.6678 metric ['0.7588', '0.6165']\n",
      "100%|██████████| 25/25 [00:01<00:00, 14.38it/s]\n",
      "2022-07-04 17:45:37.530 | INFO     | src.training.train_model:trainloop:164 - Epoch 6 train 0.6858 test 0.6509 metric ['0.7488', '0.5942']\n",
      "100%|██████████| 25/25 [00:01<00:00, 17.11it/s]\n",
      "2022-07-04 17:45:39.391 | INFO     | src.training.train_model:trainloop:164 - Epoch 7 train 0.6282 test 0.5812 metric ['0.7887', '0.6496']\n",
      "100%|██████████| 25/25 [00:01<00:00, 17.89it/s]\n",
      "2022-07-04 17:45:41.223 | INFO     | src.training.train_model:trainloop:164 - Epoch 8 train 0.5023 test 0.5157 metric ['0.8137', '0.7374']\n",
      "100%|██████████| 25/25 [00:01<00:00, 16.08it/s]\n",
      "2022-07-04 17:45:43.185 | INFO     | src.training.train_model:trainloop:164 - Epoch 9 train 0.4481 test 0.5427 metric ['0.8113', '0.7397']\n",
      "100%|██████████| 25/25 [00:01<00:00, 16.01it/s]\n",
      "2022-07-04 17:45:45.178 | INFO     | src.training.train_model:trainloop:164 - Epoch 10 train 0.4955 test 0.4502 metric ['0.8475', '0.6910']\n",
      "100%|██████████| 25/25 [00:01<00:00, 17.61it/s]\n",
      "2022-07-04 17:45:47.048 | INFO     | src.training.train_model:trainloop:164 - Epoch 11 train 0.4860 test 0.4270 metric ['0.8525', '0.7522']\n",
      "100%|██████████| 25/25 [00:01<00:00, 17.55it/s]\n",
      "2022-07-04 17:45:48.861 | INFO     | src.training.train_model:trainloop:164 - Epoch 12 train 0.4964 test 0.4572 metric ['0.8225', '0.6859']\n",
      "100%|██████████| 25/25 [00:01<00:00, 16.76it/s]\n",
      "2022-07-04 17:45:50.793 | INFO     | src.training.train_model:trainloop:164 - Epoch 13 train 0.4140 test 0.3904 metric ['0.8600', '0.7728']\n",
      "100%|██████████| 25/25 [00:01<00:00, 17.57it/s]\n",
      "2022-07-04 17:45:52.635 | INFO     | src.training.train_model:trainloop:164 - Epoch 14 train 0.3998 test 0.4260 metric ['0.8300', '0.6728']\n",
      "100%|██████████| 25/25 [00:01<00:00, 18.12it/s]\n",
      "2022-07-04 17:45:54.435 | INFO     | src.training.train_model:trainloop:164 - Epoch 15 train 0.4334 test 0.4069 metric ['0.8625', '0.7796']\n",
      "100%|██████████| 25/25 [00:01<00:00, 18.19it/s]\n",
      "2022-07-04 17:45:56.228 | INFO     | src.training.train_model:trainloop:164 - Epoch 16 train 0.3743 test 0.3772 metric ['0.8600', '0.7412']\n",
      "100%|██████████| 25/25 [00:01<00:00, 15.94it/s]\n",
      "2022-07-04 17:45:58.195 | INFO     | src.training.train_model:trainloop:164 - Epoch 17 train 0.2842 test 0.3719 metric ['0.8800', '0.7484']\n",
      "100%|██████████| 25/25 [00:01<00:00, 19.19it/s]\n",
      "2022-07-04 17:45:59.914 | INFO     | src.training.train_model:trainloop:164 - Epoch 18 train 0.2556 test 0.4525 metric ['0.8462', '0.7833']\n",
      "100%|██████████| 25/25 [00:01<00:00, 19.82it/s]\n",
      "2022-07-04 17:46:01.618 | INFO     | src.training.train_model:trainloop:164 - Epoch 19 train 0.3478 test 0.3734 metric ['0.8600', '0.7857']\n",
      "100%|██████████| 25/25 [00:01<00:00, 17.87it/s]\n",
      "2022-07-04 17:46:03.446 | INFO     | src.training.train_model:trainloop:164 - Epoch 20 train 0.3034 test 0.3189 metric ['0.9050', '0.8152']\n",
      "100%|██████████| 21/21 [00:44<00:00,  2.11s/it]\n"
     ]
    }
   ],
   "source": [
    "from src.training import train_model\n",
    "\n",
    "model = train_model.trainloop(\n",
    "    epochs=21,\n",
    "    model=model,\n",
    "    metrics=metrics,\n",
    "    optimizer=torch.optim.Adam,\n",
    "    learning_rate=1e-3,\n",
    "    loss_fn=loss_fn,\n",
    "    train_dataloader=trainstreamer,\n",
    "    test_dataloader=teststreamer,\n",
    "    log_dir=log_dir,\n",
    "    train_steps=25,\n",
    "    eval_steps=25,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save an image from the training in tensorboard in the `figures` folder.\n",
    "Explain what you are seeing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Antwoord"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In de bovenste twee grafieken van de afbeelding zijn de test en train loss te zien. Deze zijn beide nog vrij hoog en de learning rate blijft stabiel op 1e-3 ondanks dat er een scheduler in de trainloop zit die mogelijk de learning rate kan verlagen. Daarnaast vlakt de accuracy en F1Score wel iets af, echter is mijn verwachting hierbij dat het aantal epochs aan de lage kant is. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../figures/results_of_tensorboard.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Evaluate the basemodel\n",
    "Create a confusion matrix with the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Text(0.5, 15.0, 'Predicted'), Text(33.0, 0.5, 'Target')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for _ in range(10):\n",
    "    X, y = next(teststreamer)\n",
    "    yhat = model(X)\n",
    "    yhat = yhat.argmax(dim=1)\n",
    "    y_pred.append(yhat.tolist())\n",
    "    y_true.append(y.tolist())\n",
    "\n",
    "yhat = [x for y in y_pred for x in y]\n",
    "y = [x for y in y_true for x in y]\n",
    "\n",
    "cfm = confusion_matrix(y, yhat)\n",
    "cfm_norm = cfm / np.sum(cfm, axis=1, keepdims=True)\n",
    "plot = sns.heatmap(cfm_norm, annot=cfm_norm, fmt=\".3f\")\n",
    "plot.set(xlabel=\"Predicted\", ylabel=\"Target\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save this in the figures folder.\n",
    "Interpret this. \n",
    "\n",
    "- What is going on?\n",
    "- What is a good metric here?\n",
    "- how is your answer to Q1 relevant here?\n",
    "- Is there something you could do to fix/improve things, after you see these results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Antwoord"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Je ziet hier de uitkomst tussen niet normale verdeling van de dataset. Hierbij is bijvoorbeeld de eerste cel, deze is predicted 0 en voor 80% is deze ook 0. Het geeft de prestatie weer van het model en geeft aan dat deze voor label 0,1 & 2 redelijk goed kan voorspellen. Dit geldt dus niet voor groep 3, waarbij deze maar een lager getal voorspelt(tussen 50-60% ongeveer ook groep 3 voorspelt). Label 3 is de groep 'proverbs, waarbij er een veel lager aantal van is.  De dataset is dus ongebalanceerd, zoals reeds aangegeven in oefening 1. De F1Score zou kunnen zorgen voor een betere verhouding en zou kunnen worden toegevoegd als metric. Een mogelijkheid hiervoor is om data augmenting te gebruiken. Hierbij gaan we dus de data kunstmatig vergroten van de group 'proverbs'. Hierbij kun je meer gewicht toekennen aan de groep 'proverbs'. Dit zal invloed hebben op de likelihood dat een punt uit een klasse wordt getrokken, doordat je van een uniforme verdeling naar een multinomiale verdeling met gecontroleerde paremeters gaat. Daarnaast kun je ook  een andere sampling gebruiken, door gelijke aantallen van de groepen te pakken of door juist te oversampelen. \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Tune the model\n",
    "Don't overdo this.\n",
    "More is not better.\n",
    "\n",
    "Bonus points for things like:\n",
    "- Attention layers\n",
    "- Trax architecture including a functioning training loop\n",
    "\n",
    "Keep it small! It's better to present 2 or 3 sane experiments that are structured and thought trough, than 25 random guesses. You can test more, but select 2 or 3 of the best alternatives you researched, with a rationale why this works better.\n",
    "\n",
    "Keep it concise; explain:\n",
    "- what you changed\n",
    "- why you thought that was a good idea  \n",
    "- what the impact was (visualise or numeric)\n",
    "- explain the impact\n",
    "\n",
    "You dont need to get a perfect score; curiousity driven research that fails is fine.\n",
    "The insight into what is happening is more important than the quantity.\n",
    "\n",
    "Keep logs of your settings;\n",
    "either use gin, or save configs, or both :)\n",
    "Store images in the `figures` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Antwoord"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Een aantal opvallende experimenten worden verder uitgelegd. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment 1: Attention layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ik heb een nieuwe class toegevoegd in de rnn models map. Dit is de AttentionNLP > hierbij zit dus een attional layer tussen de GRU en linear. Dit zorgt voor meer context in het verhaal. In eerste instantie worden de huidige parameters ingesteld. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = rnn.AttentionNLP(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-04 17:46:50.304 | INFO     | src.data.data_tools:dir_add_timestamp:66 - Logging to ../tune/20220704-1746\n",
      "100%|██████████| 25/25 [00:04<00:00,  5.73it/s]\n",
      "2022-07-04 17:46:55.278 | INFO     | src.training.train_model:trainloop:164 - Epoch 0 train 1.2848 test 1.1532 metric ['0.3787', '0.2204']\n",
      "100%|██████████| 25/25 [00:02<00:00,  9.65it/s]\n",
      "2022-07-04 17:46:58.458 | INFO     | src.training.train_model:trainloop:164 - Epoch 1 train 1.0741 test 0.9557 metric ['0.5100', '0.3381']\n",
      "100%|██████████| 25/25 [00:01<00:00, 13.17it/s]\n",
      "2022-07-04 17:47:00.835 | INFO     | src.training.train_model:trainloop:164 - Epoch 2 train 0.9732 test 0.8540 metric ['0.6062', '0.4825']\n",
      "100%|██████████| 25/25 [00:01<00:00, 14.77it/s]\n",
      "2022-07-04 17:47:03.002 | INFO     | src.training.train_model:trainloop:164 - Epoch 3 train 0.7614 test 0.6448 metric ['0.7462', '0.6618']\n",
      "100%|██████████| 25/25 [00:01<00:00, 15.74it/s]\n",
      "2022-07-04 17:47:05.054 | INFO     | src.training.train_model:trainloop:164 - Epoch 4 train 0.6246 test 0.5531 metric ['0.7837', '0.6824']\n",
      "100%|██████████| 25/25 [00:01<00:00, 16.07it/s]\n",
      "2022-07-04 17:47:07.033 | INFO     | src.training.train_model:trainloop:164 - Epoch 5 train 0.5015 test 0.5758 metric ['0.7887', '0.6659']\n",
      "100%|██████████| 25/25 [00:01<00:00, 15.66it/s]\n",
      "2022-07-04 17:47:09.064 | INFO     | src.training.train_model:trainloop:164 - Epoch 6 train 0.5361 test 0.5088 metric ['0.8225', '0.7661']\n",
      "100%|██████████| 25/25 [00:01<00:00, 15.44it/s]\n",
      "2022-07-04 17:47:11.262 | INFO     | src.training.train_model:trainloop:164 - Epoch 7 train 0.4439 test 0.4694 metric ['0.8237', '0.7320']\n",
      "100%|██████████| 25/25 [00:01<00:00, 14.26it/s]\n",
      "2022-07-04 17:47:13.466 | INFO     | src.training.train_model:trainloop:164 - Epoch 8 train 0.4433 test 0.5226 metric ['0.8063', '0.7204']\n",
      "100%|██████████| 25/25 [00:01<00:00, 17.00it/s]\n",
      "2022-07-04 17:47:15.406 | INFO     | src.training.train_model:trainloop:164 - Epoch 9 train 0.4448 test 0.5597 metric ['0.7863', '0.7027']\n",
      "100%|██████████| 25/25 [00:01<00:00, 15.26it/s]\n",
      "2022-07-04 17:47:17.497 | INFO     | src.training.train_model:trainloop:164 - Epoch 10 train 0.4940 test 0.4800 metric ['0.8137', '0.6937']\n",
      "100%|██████████| 25/25 [00:01<00:00, 16.26it/s]\n",
      "2022-07-04 17:47:19.474 | INFO     | src.training.train_model:trainloop:164 - Epoch 11 train 0.4215 test 0.4607 metric ['0.8350', '0.7648']\n",
      "100%|██████████| 25/25 [00:01<00:00, 16.44it/s]\n",
      "2022-07-04 17:47:21.472 | INFO     | src.training.train_model:trainloop:164 - Epoch 12 train 0.3643 test 0.5577 metric ['0.7987', '0.7489']\n",
      "100%|██████████| 25/25 [00:01<00:00, 13.92it/s]\n",
      "2022-07-04 17:47:23.738 | INFO     | src.training.train_model:trainloop:164 - Epoch 13 train 0.3776 test 0.4248 metric ['0.8237', '0.7288']\n",
      "100%|██████████| 25/25 [00:01<00:00, 16.02it/s]\n",
      "2022-07-04 17:47:25.735 | INFO     | src.training.train_model:trainloop:164 - Epoch 14 train 0.3428 test 0.3820 metric ['0.8612', '0.7668']\n",
      "100%|██████████| 25/25 [00:01<00:00, 17.68it/s]\n",
      "2022-07-04 17:47:27.623 | INFO     | src.training.train_model:trainloop:164 - Epoch 15 train 0.2728 test 0.4927 metric ['0.8363', '0.7437']\n",
      "100%|██████████| 25/25 [00:01<00:00, 14.56it/s]\n",
      "2022-07-04 17:47:29.789 | INFO     | src.training.train_model:trainloop:164 - Epoch 16 train 0.3622 test 0.5501 metric ['0.7887', '0.7262']\n",
      "100%|██████████| 25/25 [00:01<00:00, 16.97it/s]\n",
      "2022-07-04 17:47:31.715 | INFO     | src.training.train_model:trainloop:164 - Epoch 17 train 0.2773 test 0.4468 metric ['0.8300', '0.7583']\n",
      "100%|██████████| 25/25 [00:01<00:00, 16.00it/s]\n",
      "2022-07-04 17:47:33.824 | INFO     | src.training.train_model:trainloop:164 - Epoch 18 train 0.3135 test 0.4587 metric ['0.8150', '0.7253']\n",
      "100%|██████████| 25/25 [00:01<00:00, 16.03it/s]\n",
      "2022-07-04 17:47:35.869 | INFO     | src.training.train_model:trainloop:164 - Epoch 19 train 0.2815 test 0.5004 metric ['0.8275', '0.7624']\n",
      "100%|██████████| 25/25 [00:01<00:00, 14.65it/s]\n",
      "2022-07-04 17:47:38.049 | INFO     | src.training.train_model:trainloop:164 - Epoch 20 train 0.3213 test 0.3949 metric ['0.8462', '0.8069']\n",
      "100%|██████████| 21/21 [00:47<00:00,  2.27s/it]\n"
     ]
    }
   ],
   "source": [
    "from src.training import train_model\n",
    "\n",
    "model = train_model.trainloop(\n",
    "    epochs=21,\n",
    "    model=model,\n",
    "    metrics=metrics,\n",
    "    optimizer=torch.optim.Adam,\n",
    "    learning_rate=1e-3,\n",
    "    loss_fn=loss_fn,\n",
    "    train_dataloader=trainstreamer,\n",
    "    test_dataloader=teststreamer,\n",
    "    log_dir=log_dir,\n",
    "    train_steps=25,\n",
    "    eval_steps=25,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Het model met de attention zorgt voor een lagere loss van de test en een hogere accuracy en F1Score. In onderstaande afbeelding zie je de oranje lijn het NLP model zonder attention layer en de blauwe lijn is het NLP model met een attention layer. Wat verder opvalt is dat het er naar uit ziet dat het model nog niet is uitgeleerd. Hierbij is de vervolgstap om de epochs te verhogen (worden verhoogd naar 40). Dit zorgt voor een verbetering (donker rode lijn). Hier zie je ook de learning rate naar beneden gaan vanwege de scheduler. De accuracy verbeterd naar 90%. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../figures/NLP_attention_tensorboard3.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment 2: Learning rate & epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " De optimizer verander ik niet direct, de Adam optimizer heeft een prijswinnende architectuur en stopt niet direct bij een mogelijk laag punt, maar kijkt verder. De learning rate en de optimizer zijn erg afhankelijk van elkaar. Met de learning rate en optimizer kun je mogelijk direct invloed uitoefenen op het model. Je ziet op het tensorboard dat de learning rate bij de attention layer nog stabiel blijft (met 21 epochs) ondanks er een scheduler in het model zit. Door de epochs te verhogen naar 40, valt op dat na 30 epochs de learning rate naar beneden gaat door middel van de scheduler (rode lijn onderstaande afbeelding). Minimaal 10 epochs wacht het model voordat deze mogelijk de learning rate verlaagd. Deze wordt dan verlaagt door middel van de factor 0.9. Om deze reden wordt het aantal epochs nog verder verhoogd naar 100. Dit kost natuurlijk wel meer process tijd en hiermee moet rekening gehouden worden met de afweging om de juiste parameters te bepalen voor het model. In onderstaande afbeelding is het verhogen van het aantal epochs weergegeven. De oranje lijn is de verhoging van de epochs naar 100, hierbij komt overfitting naar voren wat terug te zien is in de test loss. Daarnaast zie je in de learning rate grafiek dat de scheduler na ongeveer 52 epochs de learning rate met stapjes heeft verlaagd. Hierna heb ik een aantal wijzingen van de learning rate doorgevoerd ( hoger starten: 1e-2), echter zorgt dit niet voor een verbetering van het model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../figures/NLP_with_att_overfitting.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment 3: Hidden_size & batchsize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De aanpassingen van de hidden_size: Deze is op dit moment 128 en deze wil ik met kleine stapjes aanpassen. De accuracy en loss worden negatief beinvloed door minder filters(64). De dataset is niet heel groot en ik ga de filters nog verhogen naar 256. Dit zorgt er wel voor dat het model langzamer wordt en wanneer de learning rate naar beneden gaat door de scheduler stijgt de loss van de test, het aanpassen van de hidden_size  geeft geen verbetering van het model en het zorgt ook voor een hogere loss. Hierbij hou ik de hidden_size op 128, ditzelfde geldt voor de batchsizes. Onderstaande grafieken tonen de laagste losses en hoogste accuracy (90%) en F1 score (84%) waarbij het aantal epochs zijn verhoogd en de parameters t.a.v. batchsize en hidden_size gelijk zijn gebleven aan de eerste settings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../figures/fresults.PNG\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('exam-22-DDG3aTJy-py3.9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3ef7eee7c1ffccdb050f8336de9a04a9ab88c4d3eb3bee3e0a27c87a184d1d38"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
