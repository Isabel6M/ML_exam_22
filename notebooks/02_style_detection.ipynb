{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"..\")\n",
    "from src.settings import StyleSettings\n",
    "from src.data.data_tools import StyleDataset\n",
    "import numpy as np\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = StyleSettings()\n",
    "traindataset = StyleDataset([settings.trainpath])\n",
    "testdataset = StyleDataset([settings.testpath])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 419 batches in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "419"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(traindataset) // 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Lace is an openwork fabric , patterned with open holes in the work , made by machine or by hand.',\n",
       " 'wiki')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = traindataset[42]\n",
    "x, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every batch is a `Tuple[str, str]` of a sentence and a label. We can see this is a classification task.\n",
    "The task is, to classify sentences in four categories.\n",
    "Lets build a vocabulary by copy-pasting the code we used before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-03 10:25:23.765 | INFO     | src.models.tokenizer:build_vocab:27 - Found 19306 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "19308"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.models import tokenizer\n",
    "\n",
    "corpus = []\n",
    "for i in range(len(traindataset)):\n",
    "    x = tokenizer.clean(traindataset[i][0])\n",
    "    corpus.append(x)\n",
    "v = tokenizer.build_vocab(corpus, max=20000)\n",
    "len(v)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to cast the labels to an integers. You can use this dictionary to do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {\"humor\": 0, \"reuters\": 1, \"wiki\": 2, \"proverbs\": 3}\n",
    "d[y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1\n",
    "Figure out, for every class, what accuracy you should expect if the model would guess blind on the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'humor': 0.31414510476474533,\n",
       " 'wiki': 0.31175900380284843,\n",
       " 'proverbs': 0.06196405935426143,\n",
       " 'reuters': 0.3121318320781448}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import collections\n",
    "number_labels = collections.Counter([traindataset[i][1] for  i in range(len(traindataset))])\n",
    "{v: number_labels[v] / len(traindataset) for v in number_labels}\n",
    "# TODO ~ about 4 lines of code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reflect on what you see. What does this mean? What implications does this have? Why is that good/bad?\n",
    "Are there things down the line that could cause a problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naar voren komt is dat de dataset ongebalanceerd is. De verdeling van de 4 classes is niet gelijk. Het aantal zinnen met het label 'proverbs' is beduidend lager vergeleken met de andere 3 labels. De kans dat de zin het label 'humor' of 'wiki'of 'reuters' heeft is veel groter. Het model kan hierdoor niet bruikbare features leren van de groep met het label 'proverbs', het model heeft minder voorbeelden ter beschikking om van te leren. Er is hierdoor een vergrote kans van overfitting.      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2 : Implement a preprocessor\n",
    "\n",
    "We can inherit from `tokenizer.Preprocessor`\n",
    "Only thing we need to adjust is the `cast_label` function.\n",
    " \n",
    "- create a StylePreprocessor class\n",
    "- inherit from Preprocessor\n",
    "- create a new cast_label function for this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO ~ about 4 lines of code\n",
    "class StylePreprocessor(tokenizer.Preprocessor):\n",
    "    \"A preprocessor with the cast_label function for adding the labels from the dictionary\"\n",
    "    def cast_label(self, label: str) -> int:\n",
    "        if label in d.keys():\n",
    "            return d[label]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test the preprocessor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[4929,  854,   32,   15,  499,   21, 8496,  890]], dtype=torch.int32),\n",
       " tensor([2]))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor = StylePreprocessor(max=100, vocab=v, clean=tokenizer.clean)\n",
    "preprocessor([(x, y)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up the model\n",
    "We can re-use the BaseDatastreamer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data import data_tools\n",
    "\n",
    "trainstreamer = data_tools.BaseDatastreamer(\n",
    "    dataset=traindataset, batchsize=32, preprocessor=preprocessor\n",
    ").stream()\n",
    "teststreamer = data_tools.BaseDatastreamer(\n",
    "    dataset=testdataset, batchsize=32, preprocessor=preprocessor\n",
    ").stream()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 35]),\n",
       " tensor([2, 3, 3, 1, 2, 0, 0, 1, 0, 0, 0, 3, 1, 3, 1, 0, 2, 0, 0, 0, 1, 1, 1, 0,\n",
       "         2, 2, 2, 1, 1, 1, 2, 3]))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = next(trainstreamer)\n",
    "x.shape, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3 : Metrics, loss\n",
    "Select proper metrics and a loss function.\n",
    "\n",
    "Bonus: implement an additional metric function that is relevant for this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models import metrics\n",
    "import torch\n",
    "# TODO ~ 2 lines of code\n",
    "\n",
    "metrics = [metrics.Accuracy(), metrics.F1Score()]\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Om te controleren of de uitkomst correct is gebruiken we de CrossEntropyloss. Deze is geschikt voor multiclass categorisatie. MSE kan bijvoorbeeld hier niet voor gebruiken aangezien dit juist is voor numerieke voorspellingen. Ik heb als addition metric toegevoegd de F1Score toegevoegd. De f1 score kijkt naar een gemiddele van de precisie en recall ( tussen 0 en 1) scores en kan worden gebruikt voor classificatie problemen en in ongebalanceerde datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4 : Basemodel\n",
    "Create a base model. It does not need to be naive; you could re-use the\n",
    "NLP models we used for the IMDB.\n",
    "\n",
    "I suggest to start with a hidden size of about 128.\n",
    "Use a config dictionary, or a gin file, both are fine.\n",
    "\n",
    "Bonus points if you create a Trax model in src.models, and even more if you add a trax training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "log_dir = settings.log_dir\n",
    "# TODO between 2 and 8 lines of code, depending on your setup\n",
    "# Assuming you load your model in one line of code from src.models.rnn\n",
    "config = {\n",
    "    \"vocab\": len(v),\n",
    "    \"hidden_size\": 128,\n",
    "    \"num_layers\": 3, \n",
    "    \"dropout\": 0.1, \n",
    "    \"output_size\": 4,\n",
    "}\n",
    "from src.models import rnn\n",
    "model = rnn.NLPmodel(config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the trainloop.\n",
    "\n",
    "- Give the lenght of the traindataset, how many batches of 32 can you get out of it?\n",
    "- If you take a short amount of train_steps (eg 25) for every epoch, how many epochs do you need to cover the complete dataset?\n",
    "- What amount of epochs do you need to run the loop with trainsteps=25 to cover the complete traindataset once? \n",
    "- answer the questions above, and pick a reasonable epoch lenght\n",
    "\n",
    "Start with a default learning_rate of 1e-3 and an Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "419.09375"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(traindataset)/32 # lengte traindataset gedeeld door de batchsize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De lengte van de traindataset is 13.411. Hierbij kun je dus 420 batches maken van 32. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.955"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(len(traindataset) + len(testdataset)) / (32*25) # de totale lengte van de dataset gedeeld door de train step en de batchsize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De totale dataset heeft een lengte van 16.764. Om de hele datsaet te covereren zijn er 21 epochs nodig. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.76375"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(traindataset) / (32*25) # de totale lengte van de traindataset gedeeld door de train step en de batchsize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Om de totale traindataset te coveren zijn er minmaal 17 epochs nodig op basis van 25 train_steps.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-03 10:33:13.476 | INFO     | src.data.data_tools:dir_add_timestamp:66 - Logging to ../tune/20220703-1033\n",
      "100%|██████████| 25/25 [00:02<00:00,  8.48it/s]\n",
      "2022-07-03 10:33:16.934 | INFO     | src.training.train_model:trainloop:164 - Epoch 0 train 1.2951 test 1.2539 metric ['0.3113', '0.2011']\n",
      "100%|██████████| 25/25 [00:02<00:00, 11.71it/s]\n",
      "2022-07-03 10:33:19.823 | INFO     | src.training.train_model:trainloop:164 - Epoch 1 train 1.2493 test 1.2358 metric ['0.3375', '0.2185']\n",
      "100%|██████████| 25/25 [00:01<00:00, 12.85it/s]\n",
      "2022-07-03 10:33:22.419 | INFO     | src.training.train_model:trainloop:164 - Epoch 2 train 1.2212 test 1.1509 metric ['0.4525', '0.3252']\n",
      "100%|██████████| 25/25 [00:02<00:00, 12.09it/s]\n",
      "2022-07-03 10:33:24.888 | INFO     | src.training.train_model:trainloop:164 - Epoch 3 train 1.0980 test 1.0607 metric ['0.5400', '0.4061']\n",
      "100%|██████████| 25/25 [00:01<00:00, 16.21it/s]\n",
      "2022-07-03 10:33:26.842 | INFO     | src.training.train_model:trainloop:164 - Epoch 4 train 0.9879 test 0.9215 metric ['0.5450', '0.3576']\n",
      "100%|██████████| 25/25 [00:01<00:00, 16.14it/s]\n",
      "2022-07-03 10:33:28.775 | INFO     | src.training.train_model:trainloop:164 - Epoch 5 train 0.8636 test 0.8448 metric ['0.6062', '0.4508']\n",
      "100%|██████████| 25/25 [00:01<00:00, 17.11it/s]\n",
      "2022-07-03 10:33:30.740 | INFO     | src.training.train_model:trainloop:164 - Epoch 6 train 0.8298 test 0.7164 metric ['0.7550', '0.5965']\n",
      "100%|██████████| 25/25 [00:01<00:00, 17.25it/s]\n",
      "2022-07-03 10:33:32.640 | INFO     | src.training.train_model:trainloop:164 - Epoch 7 train 0.6607 test 0.6275 metric ['0.7975', '0.6442']\n",
      "100%|██████████| 25/25 [00:01<00:00, 17.64it/s]\n",
      "2022-07-03 10:33:34.449 | INFO     | src.training.train_model:trainloop:164 - Epoch 8 train 0.6498 test 0.5655 metric ['0.7950', '0.6166']\n",
      "100%|██████████| 25/25 [00:01<00:00, 15.69it/s]\n",
      "2022-07-03 10:33:36.455 | INFO     | src.training.train_model:trainloop:164 - Epoch 9 train 0.5319 test 0.5086 metric ['0.8200', '0.6702']\n",
      "100%|██████████| 25/25 [00:01<00:00, 18.30it/s]\n",
      "2022-07-03 10:33:38.245 | INFO     | src.training.train_model:trainloop:164 - Epoch 10 train 0.5463 test 0.4996 metric ['0.8213', '0.6855']\n",
      "100%|██████████| 25/25 [00:01<00:00, 18.76it/s]\n",
      "2022-07-03 10:33:39.967 | INFO     | src.training.train_model:trainloop:164 - Epoch 11 train 0.5213 test 0.4871 metric ['0.8387', '0.6987']\n",
      "100%|██████████| 25/25 [00:01<00:00, 17.12it/s]\n",
      "2022-07-03 10:33:41.884 | INFO     | src.training.train_model:trainloop:164 - Epoch 12 train 0.4838 test 0.4237 metric ['0.8550', '0.7644']\n",
      "100%|██████████| 25/25 [00:01<00:00, 17.10it/s]\n",
      "2022-07-03 10:33:43.751 | INFO     | src.training.train_model:trainloop:164 - Epoch 13 train 0.4531 test 0.4335 metric ['0.8462', '0.7455']\n",
      "100%|██████████| 25/25 [00:01<00:00, 17.58it/s]\n",
      "2022-07-03 10:33:45.552 | INFO     | src.training.train_model:trainloop:164 - Epoch 14 train 0.4749 test 0.4377 metric ['0.8413', '0.7099']\n",
      "100%|██████████| 25/25 [00:01<00:00, 18.28it/s]\n",
      "2022-07-03 10:33:47.325 | INFO     | src.training.train_model:trainloop:164 - Epoch 15 train 0.4226 test 0.3734 metric ['0.8575', '0.7587']\n",
      "100%|██████████| 25/25 [00:01<00:00, 17.18it/s]\n",
      "2022-07-03 10:33:49.166 | INFO     | src.training.train_model:trainloop:164 - Epoch 16 train 0.3741 test 0.4283 metric ['0.8438', '0.7281']\n",
      "100%|██████████| 25/25 [00:01<00:00, 18.21it/s]\n",
      "2022-07-03 10:33:50.914 | INFO     | src.training.train_model:trainloop:164 - Epoch 17 train 0.3530 test 0.3544 metric ['0.8775', '0.7837']\n",
      "100%|██████████| 25/25 [00:01<00:00, 17.05it/s]\n",
      "2022-07-03 10:33:52.804 | INFO     | src.training.train_model:trainloop:164 - Epoch 18 train 0.3423 test 0.3999 metric ['0.8700', '0.7819']\n",
      "100%|██████████| 25/25 [00:01<00:00, 19.50it/s]\n",
      "2022-07-03 10:33:54.514 | INFO     | src.training.train_model:trainloop:164 - Epoch 19 train 0.2999 test 0.4114 metric ['0.8512', '0.7687']\n",
      "100%|██████████| 25/25 [00:01<00:00, 18.74it/s]\n",
      "2022-07-03 10:33:56.202 | INFO     | src.training.train_model:trainloop:164 - Epoch 20 train 0.3270 test 0.3536 metric ['0.8775', '0.7802']\n",
      "100%|██████████| 21/21 [00:42<00:00,  2.03s/it]\n"
     ]
    }
   ],
   "source": [
    "from src.training import train_model\n",
    "\n",
    "model = train_model.trainloop(\n",
    "    epochs=21,\n",
    "    model=model,\n",
    "    metrics=metrics,\n",
    "    optimizer=torch.optim.Adam,\n",
    "    learning_rate=1e-3,\n",
    "    loss_fn=loss_fn,\n",
    "    train_dataloader=trainstreamer,\n",
    "    test_dataloader=teststreamer,\n",
    "    log_dir=log_dir,\n",
    "    train_steps=25,\n",
    "    eval_steps=25,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save an image from the training in tensorboard in the `figures` folder.\n",
    "Explain what you are seeing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../figures/tensorboard_nlp.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In de bovenste twee grafieken van de afbeelding zijn de test en train loss te zien. Deze zijn beide nog vrij hoog en de learning rate blijft stabiel op 1e-3 ondanks dat er een scheduler in de trainloop zit die mogelijk de learning rate kan verlagen. Daarnaast vlakt de accuracy wel iets af, echter is mijn verwachting hierbij dat het aantal epochs aan de lage kant is. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Evaluate the basemodel\n",
    "Create a confusion matrix with the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Text(0.5, 15.0, 'Predicted'), Text(33.0, 0.5, 'Target')]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for _ in range(10):\n",
    "    X, y = next(teststreamer)\n",
    "    yhat = model(X)\n",
    "    yhat = yhat.argmax(dim=1)\n",
    "    y_pred.append(yhat.tolist())\n",
    "    y_true.append(y.tolist())\n",
    "\n",
    "yhat = [x for y in y_pred for x in y]\n",
    "y = [x for y in y_true for x in y]\n",
    "\n",
    "cfm = confusion_matrix(y, yhat)\n",
    "cfm_norm = cfm / np.sum(cfm, axis=1, keepdims=True)\n",
    "plot = sns.heatmap(cfm_norm, annot=cfm_norm, fmt=\".3f\")\n",
    "plot.set(xlabel=\"Predicted\", ylabel=\"Target\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save this in the figures folder.\n",
    "Interpret this. \n",
    "\n",
    "- What is going on?\n",
    "- What is a good metric here?\n",
    "- how is your answer to Q1 relevant here?\n",
    "- Is there something you could do to fix/improve things, after you see these results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Je ziet hier de uitkomst tussen niet normale verdeling van de dataset. Hierbij is bijvoorbeeld de eerste cel, deze is predicted 0 en voor 80% is deze ook 0. Dit geldt dus niet voor groep 3, waarbij deze maar rond de 45% ook groep 3 voorspelt als je alleen de Accurcy als metric kiest. Label 3 is de groep 'proverbs, waarbij er een veel lager aantal van is.  De dataset is dus ongebalanceerd. Een mogelijkheid hiervoor is om data augmenting te gebruiken. Hierbij gaan we dus de data kunstmatig vergroten van de group 'proverbs'. Hierbij kun je meer gewicht toekennen aan de groep 'proverbers'. Dit zal invloed hebben op de likelihood dat een punt uit een klasse wordt getrokken, doordat je van een uniforme verdeling naar een multinomiale verdeling met gecontroleerde paremeters gaat. De F1Score zorgt voor een betere verhouding. Deze wordt toegevoegd aan de metrics en nogmaals wordt de confusion matrix uitgedraaid. Hierbij komt naar voren dat voor dat voor target 3 in ~75% van de gevallen als een 3 wordt voorspeld. Dit is beduidend hoger dan alleen gebruik te maken van de accurcy metric.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Tune the model\n",
    "Don't overdo this.\n",
    "More is not better.\n",
    "\n",
    "Bonus points for things like:\n",
    "- Attention layers\n",
    "- Trax architecture including a functioning training loop\n",
    "\n",
    "Keep it small! It's better to present 2 or 3 sane experiments that are structured and thought trough, than 25 random guesses. You can test more, but select 2 or 3 of the best alternatives you researched, with a rationale why this works better.\n",
    "\n",
    "Keep it concise; explain:\n",
    "- what you changed\n",
    "- why you thought that was a good idea  \n",
    "- what the impact was (visualise or numeric)\n",
    "- explain the impact\n",
    "\n",
    "You dont need to get a perfect score; curiousity driven research that fails is fine.\n",
    "The insight into what is happening is more important than the quantity.\n",
    "\n",
    "Keep logs of your settings;\n",
    "either use gin, or save configs, or both :)\n",
    "Store images in the `figures` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ik heb een nieuwe toegevoegd in de rnn models. Dit is de AttentionNLP > hierbij zit dus een attional layer tussen de GRU en linear. Dit zorgt voor meer context in het verhaal. In eerste instantie worden de huidige parameters ingesteld. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = rnn.AttentionNLP(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-03 11:26:07.577 | INFO     | src.data.data_tools:dir_add_timestamp:66 - Logging to ../tune/20220703-1126\n",
      "--- Logging error in Loguru Handler #1 ---\n",
      "Record was: {'elapsed': datetime.timedelta(seconds=4345, microseconds=245084), 'exception': None, 'extra': {}, 'file': (name='data_tools.py', path='/home/mladmin/code/examen-22/notebooks/../src/data/data_tools.py'), 'function': 'dir_add_timestamp', 'level': (name='INFO', no=20, icon='ℹ️'), 'line': 66, 'message': 'Logging to ../tune/20220703-1126', 'module': 'data_tools', 'name': 'src.data.data_tools', 'process': (id=2562, name='MainProcess'), 'thread': (id=140365409145472, name='MainThread'), 'time': datetime(2022, 7, 3, 11, 26, 7, 577450, tzinfo=datetime.timezone(datetime.timedelta(0), 'UTC'))}\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/mladmin/.cache/pypoetry/virtualenvs/exam-22-DDG3aTJy-py3.9/lib/python3.9/site-packages/loguru/_handler.py\", line 177, in emit\n",
      "    self._sink.write(str_record)\n",
      "  File \"/home/mladmin/.cache/pypoetry/virtualenvs/exam-22-DDG3aTJy-py3.9/lib/python3.9/site-packages/loguru/_file_sink.py\", line 175, in write\n",
      "    self._file.write(message)\n",
      "OSError: [Errno 28] No space left on device\n",
      "--- End of logging error ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('database or disk is full')).History will not be written to the database.\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 28] No space left on device: '../tune/20220703-1126'\n  In call to configurable 'trainloop' (<function trainloop at 0x7fa836fb4a60>)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m/home/mladmin/code/examen-22/notebooks/02_style_detection.ipynb Cell 46'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bml-lab-499b0cc0-f0fa-444d-a905-b2d2bead7477.westeurope.cloudapp.azure.com/home/mladmin/code/examen-22/notebooks/02_style_detection.ipynb#ch0000049vscode-remote?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtraining\u001b[39;00m \u001b[39mimport\u001b[39;00m train_model\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bml-lab-499b0cc0-f0fa-444d-a905-b2d2bead7477.westeurope.cloudapp.azure.com/home/mladmin/code/examen-22/notebooks/02_style_detection.ipynb#ch0000049vscode-remote?line=2'>3</a>\u001b[0m model \u001b[39m=\u001b[39m train_model\u001b[39m.\u001b[39;49mtrainloop(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bml-lab-499b0cc0-f0fa-444d-a905-b2d2bead7477.westeurope.cloudapp.azure.com/home/mladmin/code/examen-22/notebooks/02_style_detection.ipynb#ch0000049vscode-remote?line=3'>4</a>\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m55\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bml-lab-499b0cc0-f0fa-444d-a905-b2d2bead7477.westeurope.cloudapp.azure.com/home/mladmin/code/examen-22/notebooks/02_style_detection.ipynb#ch0000049vscode-remote?line=4'>5</a>\u001b[0m     model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bml-lab-499b0cc0-f0fa-444d-a905-b2d2bead7477.westeurope.cloudapp.azure.com/home/mladmin/code/examen-22/notebooks/02_style_detection.ipynb#ch0000049vscode-remote?line=5'>6</a>\u001b[0m     metrics\u001b[39m=\u001b[39;49mmetrics,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bml-lab-499b0cc0-f0fa-444d-a905-b2d2bead7477.westeurope.cloudapp.azure.com/home/mladmin/code/examen-22/notebooks/02_style_detection.ipynb#ch0000049vscode-remote?line=6'>7</a>\u001b[0m     optimizer\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49moptim\u001b[39m.\u001b[39;49mAdam,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bml-lab-499b0cc0-f0fa-444d-a905-b2d2bead7477.westeurope.cloudapp.azure.com/home/mladmin/code/examen-22/notebooks/02_style_detection.ipynb#ch0000049vscode-remote?line=7'>8</a>\u001b[0m     learning_rate\u001b[39m=\u001b[39;49m\u001b[39m1e-3\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bml-lab-499b0cc0-f0fa-444d-a905-b2d2bead7477.westeurope.cloudapp.azure.com/home/mladmin/code/examen-22/notebooks/02_style_detection.ipynb#ch0000049vscode-remote?line=8'>9</a>\u001b[0m     loss_fn\u001b[39m=\u001b[39;49mloss_fn,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bml-lab-499b0cc0-f0fa-444d-a905-b2d2bead7477.westeurope.cloudapp.azure.com/home/mladmin/code/examen-22/notebooks/02_style_detection.ipynb#ch0000049vscode-remote?line=9'>10</a>\u001b[0m     train_dataloader\u001b[39m=\u001b[39;49mtrainstreamer,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bml-lab-499b0cc0-f0fa-444d-a905-b2d2bead7477.westeurope.cloudapp.azure.com/home/mladmin/code/examen-22/notebooks/02_style_detection.ipynb#ch0000049vscode-remote?line=10'>11</a>\u001b[0m     test_dataloader\u001b[39m=\u001b[39;49mteststreamer,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bml-lab-499b0cc0-f0fa-444d-a905-b2d2bead7477.westeurope.cloudapp.azure.com/home/mladmin/code/examen-22/notebooks/02_style_detection.ipynb#ch0000049vscode-remote?line=11'>12</a>\u001b[0m     log_dir\u001b[39m=\u001b[39;49mlog_dir,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bml-lab-499b0cc0-f0fa-444d-a905-b2d2bead7477.westeurope.cloudapp.azure.com/home/mladmin/code/examen-22/notebooks/02_style_detection.ipynb#ch0000049vscode-remote?line=12'>13</a>\u001b[0m     train_steps\u001b[39m=\u001b[39;49m\u001b[39m25\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bml-lab-499b0cc0-f0fa-444d-a905-b2d2bead7477.westeurope.cloudapp.azure.com/home/mladmin/code/examen-22/notebooks/02_style_detection.ipynb#ch0000049vscode-remote?line=13'>14</a>\u001b[0m     eval_steps\u001b[39m=\u001b[39;49m\u001b[39m25\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bml-lab-499b0cc0-f0fa-444d-a905-b2d2bead7477.westeurope.cloudapp.azure.com/home/mladmin/code/examen-22/notebooks/02_style_detection.ipynb#ch0000049vscode-remote?line=14'>15</a>\u001b[0m )\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/exam-22-DDG3aTJy-py3.9/lib/python3.9/site-packages/gin/config.py:1605\u001b[0m, in \u001b[0;36m_make_gin_wrapper.<locals>.gin_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1603\u001b[0m scope_info \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m in scope \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(scope_str) \u001b[39mif\u001b[39;00m scope_str \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m   1604\u001b[0m err_str \u001b[39m=\u001b[39m err_str\u001b[39m.\u001b[39mformat(name, fn_or_cls, scope_info)\n\u001b[0;32m-> 1605\u001b[0m utils\u001b[39m.\u001b[39;49maugment_exception_message_and_reraise(e, err_str)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/exam-22-DDG3aTJy-py3.9/lib/python3.9/site-packages/gin/utils.py:41\u001b[0m, in \u001b[0;36maugment_exception_message_and_reraise\u001b[0;34m(exception, message)\u001b[0m\n\u001b[1;32m     39\u001b[0m proxy \u001b[39m=\u001b[39m ExceptionProxy()\n\u001b[1;32m     40\u001b[0m ExceptionProxy\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m \u001b[39m=\u001b[39m \u001b[39mtype\u001b[39m(exception)\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\n\u001b[0;32m---> 41\u001b[0m \u001b[39mraise\u001b[39;00m proxy\u001b[39m.\u001b[39mwith_traceback(exception\u001b[39m.\u001b[39m__traceback__) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/exam-22-DDG3aTJy-py3.9/lib/python3.9/site-packages/gin/config.py:1582\u001b[0m, in \u001b[0;36m_make_gin_wrapper.<locals>.gin_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1579\u001b[0m new_kwargs\u001b[39m.\u001b[39mupdate(kwargs)\n\u001b[1;32m   1581\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1582\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49mnew_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mnew_kwargs)\n\u001b[1;32m   1583\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m   1584\u001b[0m   err_str \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\n",
      "File \u001b[0;32m~/code/examen-22/notebooks/../src/training/train_model.py:134\u001b[0m, in \u001b[0;36mtrainloop\u001b[0;34m(epochs, model, optimizer, learning_rate, loss_fn, metrics, train_dataloader, test_dataloader, log_dir, train_steps, eval_steps, patience, factor, tunewriter)\u001b[0m\n\u001b[1;32m    127\u001b[0m scheduler \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mlr_scheduler\u001b[39m.\u001b[39mReduceLROnPlateau(\n\u001b[1;32m    128\u001b[0m     optimizer_,\n\u001b[1;32m    129\u001b[0m     factor\u001b[39m=\u001b[39mfactor,\n\u001b[1;32m    130\u001b[0m     patience\u001b[39m=\u001b[39mpatience,\n\u001b[1;32m    131\u001b[0m )\n\u001b[1;32m    133\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m tunewriter:\n\u001b[0;32m--> 134\u001b[0m     log_dir \u001b[39m=\u001b[39m data_tools\u001b[39m.\u001b[39;49mdir_add_timestamp(log_dir)\n\u001b[1;32m    135\u001b[0m     writer \u001b[39m=\u001b[39m SummaryWriter(log_dir\u001b[39m=\u001b[39mlog_dir)\n\u001b[1;32m    136\u001b[0m     write_gin(log_dir)\n",
      "File \u001b[0;32m~/code/examen-22/notebooks/../src/data/data_tools.py:68\u001b[0m, in \u001b[0;36mdir_add_timestamp\u001b[0;34m(log_dir)\u001b[0m\n\u001b[1;32m     66\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mLogging to \u001b[39m\u001b[39m{\u001b[39;00mlog_dir\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     67\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m log_dir\u001b[39m.\u001b[39mexists():\n\u001b[0;32m---> 68\u001b[0m     log_dir\u001b[39m.\u001b[39;49mmkdir(parents\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     69\u001b[0m \u001b[39mreturn\u001b[39;00m log_dir\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.12/lib/python3.9/pathlib.py:1323\u001b[0m, in \u001b[0;36mPath.mkdir\u001b[0;34m(self, mode, parents, exist_ok)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1320\u001b[0m \u001b[39mCreate a new directory at this given path.\u001b[39;00m\n\u001b[1;32m   1321\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1322\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1323\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_accessor\u001b[39m.\u001b[39;49mmkdir(\u001b[39mself\u001b[39;49m, mode)\n\u001b[1;32m   1324\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m:\n\u001b[1;32m   1325\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m parents \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparent \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m:\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 28] No space left on device: '../tune/20220703-1126'\n  In call to configurable 'trainloop' (<function trainloop at 0x7fa836fb4a60>)"
     ]
    }
   ],
   "source": [
    "from src.training import train_model\n",
    "\n",
    "model = train_model.trainloop(\n",
    "    epochs=55,\n",
    "    model=model,\n",
    "    metrics=metrics,\n",
    "    optimizer=torch.optim.Adam,\n",
    "    learning_rate=1e-3,\n",
    "    loss_fn=loss_fn,\n",
    "    train_dataloader=trainstreamer,\n",
    "    test_dataloader=teststreamer,\n",
    "    log_dir=log_dir,\n",
    "    train_steps=25,\n",
    "    eval_steps=25,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Het model met de attention zorgt voor een lagere loss van de test en een hogere accuracy en F1Score. In onderstaande afbeelding zie je de oranje lijn het NLP model zonder attention layer en de blauwe lijn is het NLP model met een attention layer. Wat verder opvalt is dat het er naar uit ziet dat het model nog niet is uitgeleerd. Hierbij is de vervolgstap om de epochs te verhogen (worden verhoogd naar 40). Dit zorgt voor een verbetering (donker rode lijn). Hier zie je ook de learning rate naar beneden gaan vanwege de scheduler. De accuracy verbeterd naar 90%. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../figures/NLP_attention_tensorboard3.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " De optimizer verander ik niet direct, de Adam optimizer heeft een prijswinnende architectuur en stopt niet direct bij een mogelijk laag punt, maar kijkt verder. De learning rate en de optimizer zijn erg afhankelijk van elkaar. Met de learning rate en optimizer kun je mogelijk direct invloed uitoefenen op het model. Je ziet op het tensorboard dat de learning rate bij de attention layer nog stabiel blijft (met 21 epochs) ondanks er een scheduler in het model zit. Door de epochs te verhogen naar 40, valt op dat na 30 epochs de learning rate naar beneden gaat door middel van de scheduler (rode lijn onderstaande afbeelding). Minimaal 10 epochs wacht het model voordat deze mogelijk de learning rate verlaagd. Deze wordt dan verlaagt door middel van de factor 0.9. Om deze reden wordt het aantal epochs nog verder verhoogd naar 100. Dit kost natuurlijk wel meer process tijd en hiermee moet rekening gehouden worden met de afweging om de juiste parameters te bepalen voor het model. In onderstaande afbeelding is het verhogen van het aantal epochs weergegeven. De oranje lijn is de verhoging van de epochs naar 100, hierbij komt overfitting naar voren wat terug te zien is in de test loss. Daarnaast zie je in de learning rate grafiek dat de scheduler na ongeveer 52 epochs de learning rate met stapjes heeft verlaagd. Hierna heb ik een aantal wijzingen van de learning rate doorgevoerd ( hoger starten: 1e-2), echter zorgt dit niet voor een verbetering van het model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../figures/NLP_with_att_overfitting.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De aanpassingen van de hidden_size: Deze is op dit moment 128 en deze wil ik met kleine stapjes aanpassen. De accuracy en loss worden negatief beinvloed door minder filters(64). De dataset is niet heel groot en ik ga de filters nog verhogen naar 256. Dit zorgt er wel voor dat het model langzamer wordt en wanneer de learning rate naar beneden gaat door de scheduler stijgt de loss van de test, het aanpassen van de hidden_size  geeft geen verbetering van het model en het zorgt ook voor een hogere loss. Hierbij hou ik de hidden_size op 128, ditzelfde geldt voor de batchsizes. Onderstaande grafieken tonen de laagste losses en hoogste accuracy (90%) en F1 score (84%) waarbij het aantal epochs zijn verhoogd en de parameters t.a.v. batchsize en hidden_size gelijk zijn gebleven aan de eerste settings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../figures/fresults.PNG\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('exam-22-DDG3aTJy-py3.9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3ef7eee7c1ffccdb050f8336de9a04a9ab88c4d3eb3bee3e0a27c87a184d1d38"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
